{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project By Raunaq Singh Jabbal\n",
    "\n",
    "Project made considering time constraints, is easy to read and understand, with less LOC's and model has fast inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver         \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "import contractions\n",
    "import regex as re\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Scraping and PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def preprocess(text):\n",
    "    text = unidecode.unidecode(text)    # Remove accents\n",
    "    text = contractions.fix(text)       # Expand contractions\n",
    "    text = text.lower()                 # Convert to lowercase\n",
    "    # re.sub(r'\\d+', '', text)          # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = text.strip()                 # Remove extra whitespaces\n",
    "    text = \" \".join(text.split())       \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Ajio Web Scraping\n",
    "\n",
    "Ajio used as it has an average sized collection (~53K), where I use only women's section, as apparels have a lot of variety.\n",
    "\n",
    "Description of most products only has information like dimensions of the product, quantity of the product, etc, hence that information has not been scraped.\n",
    "\n",
    "Data has been scraped from the below categories:\n",
    "- Caps & Hats\n",
    "- Dresses\n",
    "- Gloves, Scarves & Bandanas\n",
    "- Jackets & Shrugs\n",
    "- Jeans & Leggings\n",
    "- Jumpsuits & Playsuits\n",
    "- Kurta Suit Sets\n",
    "- Kurtas & Kurtis\n",
    "- Leggings\n",
    "- Lehenga Choli Sets\n",
    "- Lounge Tops & Sweatshirts\n",
    "- Maternity Wear \n",
    "- Night & Lounge Sets\n",
    "- Pyjama & Lounge Sets\n",
    "- Salwars & Churidars\n",
    "- Sarees\n",
    "- Shapewear\n",
    "- Shirts\n",
    "- Shirts, Tops & Tunics\n",
    "- Shorts\n",
    "- Skirts & Ghagras\n",
    "- Sweaters & SweatShirts\n",
    "- Trackpants\n",
    "- trousers and Pants\n",
    "- Tshirts\n",
    "\n",
    "Total extracted data: ~6K samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [14:45<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.ajio.com/search/?query=%3Arelevance%3Agenderfilter%3AWomen%3Al1l3nestedcategory%3AWomen%20-%20Dresses%3Al1l3nestedcategory%3AWomen%20-%20Jackets%20%26%20Shrugs%3Al1l3nestedcategory%3AWomen%20-%20Jeans%20%26%20Jeggings%3Al1l3nestedcategory%3AWomen%20-%20Jumpsuits%20%26%20Playsuits%3Al1l3nestedcategory%3AWomen%20-%20Kurta%20Suit%20Sets%3Al1l3nestedcategory%3AWomen%20-%20Kurtas%20%26%20Kurtis%3Al1l3nestedcategory%3AWomen%20-%20Leggings%3Al1l3nestedcategory%3AWomen%20-%20Lehenga%20Choli%20Sets%3Al1l3nestedcategory%3AWomen%20-%20Lounge%20Tops%20%26%20Sweatshirts%3Al1l3nestedcategory%3AWomen%20-%20Maternity%20Wear%3Al1l3nestedcategory%3AWomen%20-%20Night%20%26%20Lounge%20Sets%3Al1l3nestedcategory%3AWomen%20-%20Pyjamas%20%26%20Lounge%20Shorts%3Al1l3nestedcategory%3AWomen%20-%20Salwars%20%26%20Churidars%3Al1l3nestedcategory%3AWomen%20-%20Sarees%3Al1l3nestedcategory%3AWomen%20-%20Shapewear%3Al1l3nestedcategory%3AWomen%20-%20Shirts%2C%20Tops%20%26%20Tunics%3Al1l3nestedcategory%3AWomen%20-%20Shirts%3Al1l3nestedcategory%3AWomen%20-%20Shorts%3Al1l3nestedcategory%3AWomen%20-%20Skirts%20%26%20Ghagras%3Al1l3nestedcategory%3AWomen%20-%20Sweaters%20%26%20Sweatshirts%3Al1l3nestedcategory%3AWomen%20-%20Track%20Pants%3Al1l3nestedcategory%3AWomen%20-%20Trousers%20%26%20Pants%3Al1l3nestedcategory%3AWomen%20-%20Tshirts%3Al1l3nestedcategory%3AWomen%20-%20Caps%20%26%20Hats%3Al1l3nestedcategory%3AWomen%20-%20Gloves%2C%20Scarves%20%26%20Bandanas&text=clothes&gridColumns=3&segmentIds=\"\n",
    "mainlink = \"https://www.ajio.com\"   # Main link for the website\n",
    "browser = webdriver.Edge()          # Using Edge browser for Selenium\n",
    "browser.get(link)                   # Open the link in browser\n",
    "\n",
    "for i in tqdm(range(0,540000,1000)):                            # Infinite Scroll to get all the products\n",
    "\tbrowser.execute_script(\"window.scrollTo(0,\" + str(i) + \")\") # Scroll down the page to get to the bottom\n",
    "\ttime.sleep(.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/540 [05:00<3:00:51, 20.71s/it]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,540000,1000)):                            # Infinite Scroll to get all the products\n",
    "\tbrowser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\") # Scroll down the page to get to the bottom\n",
    "\ttime.sleep(.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Scraping:\n",
    "<img src=\"title1.png\" height=400 width=600/>\n",
    "\n",
    "Link Scraping:\n",
    "<img src=\"link1.png\" height=400 width=600 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(browser.page_source,'html.parser')             # Get the page source\n",
    "\n",
    "titles1=[]\n",
    "links1=[]\n",
    "\n",
    "for i in soup.find_all('div',class_='nameCls'):                     # Get the titles of the products\n",
    "    titles1.append(i.find('div',class_='name').text)\n",
    "\n",
    "for i in soup.find_all('a',class_='rilrtl-products-list__link'):    # Get the links of the products\n",
    "    links1.append(mainlink+i.attrs['href'])\n",
    "    \n",
    "titles1 = titles1.astype(str)                                       # Convert to string                    \n",
    "links1 = links1.astype(str)                                         # Convert to string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. ShoppersStop Web Scraping\n",
    "\n",
    "Good site for extracting apparel information for men (~23K samples)\n",
    "\n",
    "I will extract information about the top 3000 apparel with price range of 1001-2000 from the following categories:\n",
    "\n",
    "- Ethnic wear\n",
    "- Jeans\n",
    "- Shirts\n",
    "- Shorts\n",
    "- T-shirts & Polos\n",
    "- Trousers\n",
    "- Winter wear\n",
    "\n",
    "Total extracted data: ~3K samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link2 = \"https://www.shoppersstop.com/search/?q=clothes%3Arelevance%3AinStockFlag%3Atrue%3AcategoryName%3AClothing%3Al3category%3AEthnicwear%3Al3category%3AJeans%3Al3category%3AWinterwear%3Aprice%3A1001-2000%3Al3category%3AShirts%3Al3category%3ATrousers%3Al3category%3AShorts%3Al3category%3AT-Shirts%2B%2526%2BPolos&text=clothes&startRange=&endRange=&brandPageId=&linkText=#\"\n",
    "mainlink2 = \"https://www.shoppersstop.com\"   # Main link for the website\n",
    "browser2 = webdriver.Edge()          # Using Edge browser for Selenium\n",
    "browser2.get(link)                   # Open the link in browser\n",
    "\n",
    "for i in tqdm(range(0,540000,1000)):                            # Infinite Scroll to get all the products\n",
    "\tbrowser2.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\") # Scroll down the page to get to the bottom\n",
    "\ttime.sleep(.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Scraping:\n",
    "<img src=\"title1.png\" height=400 width=600/>\n",
    "\n",
    "Link Scraping:\n",
    "<img src=\"link1.png\" height=400 width=600 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(browser2.page_source,'html.parser')             # Get the page source\n",
    "\n",
    "titles2=[]\n",
    "links2=[]\n",
    "\n",
    "for i in soup.find_all('div',class_='nameCls'):                     # Get the titles of the products\n",
    "    titles2.append(i.find('div',class_='name').text)\n",
    "\n",
    "for i in soup.find_all('a',class_='rilrtl-products-list__link'):    # Get the links of the products\n",
    "    links2.append(mainlink+i.attrs['href'])\n",
    "    \n",
    "titles2 = titles2.astype(str)                                       # Convert to string                    \n",
    "links2 = links2.astype(str)                                         # Convert to string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measure Similarity\n",
    "\n",
    "We need some form of sentence embedding so that we can figure out the most relevant samples from our dataset that match the description given by the user. We also need the most advanced and efficient algorithm to solve this problem.\n",
    "\n",
    "\n",
    "Talking about BERT, it solves semantic search in a pair wise fashion. The major downside with using BERT here is if we have input given by the user, the titles in our dataset have to be given everytime to the model to compute similarity which is  computationally intensive.\n",
    "\n",
    "Now, Siamese Networks contain 2 or more identical networks that share the same weight in parallel and are usually used to find out similarity. Siamese networks don't have to train with every possible combination unlike BERT.\n",
    "\n",
    "Sentence BERT is like the cross-encoder BERT, but it doesn't have the classification/similarity calculation head. It uses a Siamese architecture containing 2 BERT architectures.\n",
    "\n",
    "We provide 2 sentences, one to each BERT instance, output of which an embedding which is mean pooled. For training, the 2 embeddings are concatenated and passed through a softmax layer for classification. While testing, the cosine similarity of the 2 embeddings is the output.\n",
    "\n",
    "Sentence Transformers is a Python framework for state of the art sentence, text, and image embedddings based on PyTorch.\n",
    "\n",
    "- We used a pretained SBERT with the latest \"all-mpnet-base-v2\" model which outputs a 384 dimensional normalised embedding vector trained on 1 billion training pairs.\n",
    "\n",
    "- Save the embeddings of the corresponding titles in a csv files so that we don't have to keep recomputing those values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10                                                                            # Number of items to be displayed\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')                                # Loading the Sentence Transformer Model\n",
    "dataset = pd.read_csv('ajio.csv')                                               # Loading the Datastore from the CSV file\n",
    "embeddings = model.encode(dataset[\"Title\"], convert_to_tensor=True)             # Getting the embeddings of the Search Query\n",
    "pd.DataFrame(embeddings.numpy()).to_csv('embeddings.csv', index=False)          # Saving the embeddings in a CSV file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function\n",
    "\n",
    "- Load the dataset including the titles and links and the vector embeddings \n",
    "- Preprocess input given by the user\n",
    "- Use the SBERT model to obtain it's embedding vector\n",
    "- Calculate cosine similarity of that vector with all the vectors in the embeddings\n",
    "- Return the N highest links \n",
    "\n",
    "In most cases, it takes less than 2 seconds.\n",
    "We can still improve on this time, by using simpler SBERT pretrained models and using techniquees like PCA as we don't need a 384 dimensional long embedding vector for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "import contractions\n",
    "import regex as re\n",
    "import os \n",
    "\n",
    "N=10\n",
    "\n",
    "def preprocess(text):\n",
    "    text = unidecode.unidecode(text)    # Remove accents\n",
    "    text = contractions.fix(text)       # Expand contractions\n",
    "    text = text.lower()                 # Convert to lowercase\n",
    "    # re.sub(r'\\d+', '', text)          # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = text.strip()                 # Remove extra whitespaces\n",
    "    text = \" \".join(text.split())       \n",
    "    return text\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')                                # Loading the Sentence Transformer Model\n",
    "\n",
    "def main(sample):\n",
    "    t= time.time()\n",
    "    ajio_embeddings = pd.read_csv('embeddings.csv').to_numpy()                      # Loading the embeddings from the CSV file\n",
    "    ajio_embeddings = torch.tensor(ajio_embeddings, dtype=torch.float32)            # Converting the embeddings to a tensor\n",
    "    ajio = pd.read_csv('dataset.csv')                                               # Loading the Datastore from the CSV file\n",
    "\n",
    "    sample = \"embroidered gown full length\"                                         # Sample Search Query\n",
    "    sample = preprocess(sample)                                                     # Preprocessing the Search Query\n",
    "    sample_emb = model.encode(sample, convert_to_tensor=True)                       # Getting the embeddings of the Search Query\n",
    "\n",
    "    scores = np.array(util.cos_sim(sample_emb, ajio_embeddings).tolist()[0])        # Getting the cosine similarity scores of the Search Query with all the items\n",
    "    indexes = scores.argsort()[::-1][:N]                                            # Getting the indexes of the top N items\n",
    "    urls= np.array(ajio.iloc[indexes,1])                                            # Getting the links of the top N items\n",
    "    # print(np.array(list(zip(urls,scores[indexes]))))                                # Printing the links and the scores of the top N items  \n",
    "\n",
    "    print(\"Time Taken: \", time.time()-t)\n",
    "    print(urls)\n",
    "    return urls\n",
    "\n",
    "main(\"embroidered party wear saree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
